{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"<h1>Title Goes Here</h1>\n",
    "<b>Bolded Text</b>\n",
    "<i>Italicized Text</i>\n",
    "<img src=\"this should all be gone\"/>\n",
    "<a href=\"this will be gone, too\">But this will still be here!</a>\n",
    "I run. He ran. She is running. Will they stop running?\n",
    "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
    "[Some text we don't want to keep is in here]\n",
    "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
    "something... is! wrong() with.,; this :: sentence.\n",
    "I can't do this anymore. I didn't know them. Why couldn't you have dinner at the restaurant?\n",
    "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
    "Don't do it.... Just don't. Billy! I know what you're doing. This is a great little house you've got here.\n",
    "[This is some other unwanted text]\n",
    "John: \"Well, well, well.\"\n",
    "James: \"There, there. There, there.\"\n",
    "&nbsp;&nbsp;\n",
    "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
    "I have to go get 2 tutus from 2 different stores, too.\n",
    "22    45   1067   445\n",
    "{{Here is some stuff inside of double curly braces.}}\n",
    "{Here is more stuff in single curly braces.}\n",
    "[DELETE]\n",
    "</body>\n",
    "</html>\"\"\""
   ]
  },
  {
   "source": [
    "## Remoção de Ruído\n",
    "\n",
    "Nesse processo de remoção de ruídos faremos:\n",
    " - Remoção do cabeçalho e do rodapé\n",
    " - Remoção dos códigos HTML e XML\n",
    " - Extração de dados importantes de outros formatos, como JSON"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Na célula seguinte é utilizada a biblioteca Beautiful Soup para a remoção das tags HTML, através do parser html que esta possui\n",
    "\n",
    "Na segunda função é feita a remoção de colchetes duplos (\"[[]]\") utilizando expressões regulares e remove-se todo texto dentro dos colchetes duplos. Essa remoção é específica deste exemplo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = denoise_text(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'Title Goes Here\\nBolded Text\\nItalicized Text\\n\\nBut this will still be here!\\nI run. He ran. She is running. Will they stop running?\\nI talked. She was talking. They talked to them about running. Who ran to the talking runner?\\n\\n¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\\nsomething... is! wrong() with.,; this :: sentence.\\nI can\\'t do this anymore. I didn\\'t know them. Why couldn\\'t you have dinner at the restaurant?\\nMy favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\\nDon\\'t do it.... Just don\\'t. Billy! I know what you\\'re doing. This is a great little house you\\'ve got here.\\n\\nJohn: \"Well, well, well.\"\\nJames: \"There, there. There, there.\"\\n\\xa0\\xa0\\nThere are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\\nI have to go get 2 tutus from 2 different stores, too.\\n22    45   1067   445\\n{{Here is some stuff inside of double curly braces.}}\\n{Here is more stuff in single curly braces.}\\n\\n\\n'"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display(sample)"
   ]
  },
  {
   "source": [
    "Agora iremos remover as contrações. Em inglês, encontramos diversos textos, sendo eles formais ou informais, que possuem contrações como _didn't_ ou _don't_. Ao utilizar um tokenizer, ou seja, ao separar nossas palavras nos passos a frente, essas contrações serão extraídas de forma que irão inserir ruídos em nossos dados. Uma contração como _didn't_ seria transformada em dois tokens (\"did\" e \"n't\"). Então para isso, removemos as contrações, transformando em duas palavras diferentes: did e not. Para isso iremos utilizar a biblioteca contractions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text):\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_contractions_fixed = replace_contractions(sample) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Title Goes Here\\nBolded Text\\nItalicized Text\\n\\nBut this will still be here!\\nI run. He ran. She is running. Will they stop running?\\nI talked. She was talking. They talked to them about running. Who ran to the talking runner?\\n\\n¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\\nsomething... is! wrong() with.,; this :: sentence.\\nI can not do this anymore. I did not know them. Why could not you have dinner at the restaurant?\\nMy favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\\ndo not do it.... Just do not. Billy! I know what you are doing. This is a great little house you have got here.\\n\\nJohn: \"Well, well, well.\"\\nJames: \"There, there. There, there.\"\\n\\xa0\\xa0\\nThere are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\\nI have to go get 2 tutus from 2 different stores, too.\\n22    45   1067   445\\n{{Here is some stuff inside of double curly braces.}}\\n{Here is more stuff in single curly braces.}\\n\\n\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "sample_contractions_fixed"
   ]
  },
  {
   "source": [
    "## Tokenization\n",
    "\n",
    "Ao finalizar nossa remoção de ruídos, agora iremos para o processo de Tokenization. Basicamente, tokenization é o processo de segmentar uma grande quantidade de texto em elementos menores, sejam esses elementos parágrafos, sentenças ou palavras. Normalmente tokenization se refere à segmentação do texto em palavras, enquanto segmentação refere-se à particionar o texto em elementos maiores que uma palavra. Normalmente um processamento mais profundo do texto é feito apenas após o processo de segmentação."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}