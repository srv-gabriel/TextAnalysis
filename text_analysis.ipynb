{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"<h1>Title Goes Here</h1>\n",
    "<b>Bolded Text</b>\n",
    "<i>Italicized Text</i>\n",
    "<img src=\"this should all be gone\"/>\n",
    "<a href=\"this will be gone, too\">But this will still be here!</a>\n",
    "I run. He ran. She is running. Will they stop running?\n",
    "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
    "[Some text we don't want to keep is in here]\n",
    "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
    "something... is! wrong() with.,; this :: sentence.\n",
    "I can't do this anymore. I didn't know them. Why couldn't you have dinner at the restaurant?\n",
    "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
    "Don't do it.... Just don't. Billy! I know what you're doing. This is a great little house you've got here.\n",
    "[This is some other unwanted text]\n",
    "John: \"Well, well, well.\"\n",
    "James: \"There, there. There, there.\"\n",
    "&nbsp;&nbsp;\n",
    "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
    "I have to go get 2 tutus from 2 different stores, too.\n",
    "22    45   1067   445\n",
    "{{Here is some stuff inside of double curly braces.}}\n",
    "{Here is more stuff in single curly braces.}\n",
    "[DELETE]\n",
    "</body>\n",
    "</html>\"\"\""
   ]
  },
  {
   "source": [
    "## Remoção de Ruído\n",
    "\n",
    "Nesse processo de remoção de ruídos faremos:\n",
    " - Remoção do cabeçalho e do rodapé\n",
    " - Remoção dos códigos HTML e XML\n",
    " - Extração de dados importantes de outros formatos, como JSON"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Na célula seguinte é utilizada a biblioteca Beautiful Soup para a remoção das tags HTML, através do parser html que esta possui\n",
    "\n",
    "Na segunda função é feita a remoção de colchetes duplos (\"[[]]\") utilizando expressões regulares e remove-se todo texto dentro dos colchetes duplos. Essa remoção é específica deste exemplo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = denoise_text(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'Title Goes Here\\nBolded Text\\nItalicized Text\\n\\nBut this will still be here!\\nI run. He ran. She is running. Will they stop running?\\nI talked. She was talking. They talked to them about running. Who ran to the talking runner?\\n\\n¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\\nsomething... is! wrong() with.,; this :: sentence.\\nI can\\'t do this anymore. I didn\\'t know them. Why couldn\\'t you have dinner at the restaurant?\\nMy favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\\nDon\\'t do it.... Just don\\'t. Billy! I know what you\\'re doing. This is a great little house you\\'ve got here.\\n\\nJohn: \"Well, well, well.\"\\nJames: \"There, there. There, there.\"\\n\\xa0\\xa0\\nThere are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\\nI have to go get 2 tutus from 2 different stores, too.\\n22    45   1067   445\\n{{Here is some stuff inside of double curly braces.}}\\n{Here is more stuff in single curly braces.}\\n\\n\\n'"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display(sample)"
   ]
  },
  {
   "source": [
    "Agora iremos remover as contrações. Em inglês, encontramos diversos textos, sendo eles formais ou informais, que possuem contrações como _didn't_ ou _don't_. Ao utilizar um tokenizer, ou seja, ao separar nossas palavras nos passos a frente, essas contrações serão extraídas de forma que irão inserir ruídos em nossos dados. Uma contração como _didn't_ seria transformada em dois tokens (\"did\" e \"n't\"). Então para isso, removemos as contrações, transformando em duas palavras diferentes: did e not. Para isso iremos utilizar a biblioteca contractions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text):\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_contractions_fixed = replace_contractions(sample) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Title Goes Here\\nBolded Text\\nItalicized Text\\n\\nBut this will still be here!\\nI run. He ran. She is running. Will they stop running?\\nI talked. She was talking. They talked to them about running. Who ran to the talking runner?\\n\\n¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\\nsomething... is! wrong() with.,; this :: sentence.\\nI can not do this anymore. I did not know them. Why could not you have dinner at the restaurant?\\nMy favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\\ndo not do it.... Just do not. Billy! I know what you are doing. This is a great little house you have got here.\\n\\nJohn: \"Well, well, well.\"\\nJames: \"There, there. There, there.\"\\n\\xa0\\xa0\\nThere are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\\nI have to go get 2 tutus from 2 different stores, too.\\n22    45   1067   445\\n{{Here is some stuff inside of double curly braces.}}\\n{Here is more stuff in single curly braces.}\\n\\n\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "sample_contractions_fixed"
   ]
  },
  {
   "source": [
    "## Tokenization\n",
    "\n",
    "Ao finalizar nossa remoção de ruídos, agora iremos para o processo de Tokenization. Basicamente, tokenization é o processo de segmentar uma grande quantidade de texto em elementos menores, sejam esses elementos parágrafos, sentenças ou palavras. Normalmente tokenization se refere à segmentação do texto em palavras, enquanto segmentação refere-se à particionar o texto em elementos maiores que uma palavra. Normalmente um processamento mais profundo do texto é feito apenas após o processo de segmentação.\n",
    "\n",
    "E para tokenizar é bem simples. Utilizando o NLTK (Natural Language ToolKit) executamos a função word_tokenize na nossa amostra de texto. Lembrando que o NLTK possui alguns downloads adicionais que devem ser feitos para a execução de certas funções, então na execução das funções dessa biblioteca podem ser disparados erros, porém basta apenas ler os erros e verificar qual download deve ser feito."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(sample_contractions_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "source": [
    "## Normalization\n",
    "\n",
    "O processo de normalização consiste em uma série de tarefas para colocar todas as palavras em \"grupos\" similares. Exemplos dessas tarefas podem ser: remoção de pontuação, transformação de maiúsculo para minúsculo, remoção de sufixos e prefixos e assim por diante. Dessa forma, conseguimos reduzir o número de palavras e obter apenas palavras úteis para nosso problema. Nesse notebook iremos focar em 3 processos de normalização."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming consiste na remoção de afixos (sufixos, prefixos, infixos, circunfixos) com o intuito de obter apenas a raiz da palavra. Este processo tem o intuito de reduzir a quantidade de diferentes palavras que apresentam o mesmo significado ou \"objetivo\". Por exemplo, palavras como _running_ ou _runner_ são convertidas para _run_. Para esse processo iremos utilizar novamente a biblioteca NLTK, com a função LancasterStemmer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(words):\n",
    "    \n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems"
   ]
  },
  {
   "source": [
    "## Lemmatization\n",
    "\n",
    "O processo de lemmatization é bem mais sofisticado. Esse processo busca extrair a forma canônica das palavras, que vai além de apenas remover afixos. Por exemplo, a palavra melhor é transformada em bom, pior em ruim, corredor em correr, e assim por diante. É um processo parecido com Stemming, porém stemming poderia transformar de formas erradas algumas palavras. Se pegássemos a palavra _better_ e retirássemos apenas seu sufixo ficaríamos com a palavra _bet_, enquanto se aplicamos o lemmatization iremos ficar com a palavra _good_."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_verbs(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "source": [
    "## Todo o resto\n",
    "\n",
    "Devido a este notebook tratar de uma introdução ao pré-processamento de texto e por lemmatization e stemming se tratarem de processos extremamente importantes e com certo grau de sofisticação, se fazendo necessário conhecimento de linguagem para compreendê-los, esses dois métodos principais foram abordados separadamente.\n",
    "\n",
    "Porém agora vamos lidar com técnicas mais gerais de limpeza do texto, agregando todas elas no mesmo contexto. Lembrando que as técnicas utilizadas a seguir também são essenciais para o resultado futuro, entretanto envolvem operações básicas de substituição e remoção nos textos.\n",
    "\n",
    "E essas tarefas são:\n",
    " - Transformar todas as letras para minúsculas\n",
    " - Remover números ou escreve-los por extenso\n",
    " - Remover pontuação\n",
    " - Remover espaços em branco\n",
    " - Remover _stop-words_ padrão\n",
    "\n",
    "_Stop Words_ são palavras que não têm nenhum valor semântico que possa cooperar para a resolução do problema, como por exemplo: artigos, conjunções, preposições, etc. São palavras que não agregam valor ao modelo e podem se tornar ruído, adicionando mais uma dimensão ao modelo e atrapalhando a classificação. Há também a existência de _stop words_ específicas da tarefa que está sendo executada. Por exemplo, em textos de redes sociais, caso estejamos procurando extrair o conteúdo de um post, seria uma boa alternativa utilizar algumas gírias como _stop words_, como as palavras \"tipo\" ou \"caramba\", que irão se repetir constantemente mas que não agregam ao conteúdo do texto.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = normalize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems, lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems, lemmas = stem_and_lemmatize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['titl', 'goe', 'bold', 'text', 'it', 'text', 'stil', 'run', 'ran', 'run', 'stop', 'run', 'talk', 'talk', 'talk', 'run', 'ran', 'talk', 'run', 'sebast', 'nicola', 'alejandro', 'jeronimo', 'going', 'stor', 'tomorrow', 'morn', 'someth', 'wrong', 'sent', 'anym', 'know', 'could', 'din', 'resta', 'favorit', 'movy', 'franch', 'ord', 'indian', 'jon', 'marvel', 'cinem', 'univers', 'star', 'war', 'back', 'fut', 'harry', 'pot', 'bil', 'know', 'gre', 'littl', 'hous', 'got', 'john', 'wel', 'wel', 'wel', 'jam', 'lot', 'reason', 'one hundred and on', 'reason', 'one million', 'reason', 'act', 'go', 'get', 'two', 'tut', 'two', 'diff', 'stor', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred and forty-five', 'stuff', 'insid', 'doubl', 'cur', 'brac', 'stuff', 'singl', 'cur', 'brac']\n"
     ]
    }
   ],
   "source": [
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['title', 'go', 'bolded', 'text', 'italicize', 'text', 'still', 'run', 'run', 'run', 'stop', 'run', 'talk', 'talk', 'talk', 'run', 'run', 'talk', 'runner', 'sebastian', 'nicolas', 'alejandro', 'jeronimo', 'go', 'store', 'tomorrow', 'morning', 'something', 'wrong', 'sentence', 'anymore', 'know', 'could', 'dinner', 'restaurant', 'favorite', 'movie', 'franchise', 'order', 'indiana', 'jones', 'marvel', 'cinematic', 'universe', 'star', 'war', 'back', 'future', 'harry', 'potter', 'billy', 'know', 'great', 'little', 'house', 'get', 'john', 'well', 'well', 'well', 'jam', 'lot', 'reason', 'one hundred and one', 'reason', 'one million', 'reason', 'actually', 'go', 'get', 'two', 'tutus', 'two', 'different', 'store', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred and forty-five', 'stuff', 'inside', 'double', 'curly', 'brace', 'stuff', 'single', 'curly', 'brace']\n"
     ]
    }
   ],
   "source": [
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}